# Mistral.rs Configuration File

[server]
host = "0.0.0.0"
port = 8080

[model]
path = "/models"
default_model = "mistral-7b-instruct"

[inference]
max_batch_size = 8
max_sequence_length = 4096
temperature = 0.7

[logging]
level = "info"
format = "json"

[resources]
num_threads = 8
use_gpu = true