# Mistral 7B Instruct v0.2 Configuration

[model]
id = "mistral:7b"
path = "mistral-7b-instruct-v0.2.gguf"
architecture = "mistral"
size = "7B"
quantization = "q8_0"
context_length = 32768
supports_functions = false

[inference]
temperature = 0.7
top_p = 0.95
top_k = 40
repeat_penalty = 1.1
max_tokens = 2048
threads = 0
batch_size = 512
use_gpu = true
gpu_layers = -1

[prompt]
# Mistral Instruct format
system_template = "<s>[INST] {system_message}\n\n"
user_template = "{user_message} [/INST]"
assistant_template = "{assistant_message}</s>"
eos_token = "</s>"
bos_token = "<s>"

[lora]
enabled = true
adapter_path = ""
scale = 1.0

[memory]
max_model_memory = 0
max_cache_memory = 0
use_mmap = true
lock_memory = false

[performance]
continuous_batching = true
flash_attention = true
tensor_parallel = false
tensor_parallel_size = 1

[logging]
level = "info"
log_stats = true
log_prompts = false